
@article{martis_ecg_2013,
	title = {{ECG} beat classification using {PCA}, {LDA}, {ICA} and {Discrete} {Wavelet} {Transform}},
	volume = {8},
	issn = {1746-8094},
	url = {http://www.sciencedirect.com/science/article/pii/S1746809413000062},
	doi = {10.1016/j.bspc.2013.01.005},
	abstract = {Electrocardiogram (ECG) is the P-QRS-T wave, representing the cardiac function. The information concealed in the ECG signal is useful in detecting the disease afflicting the heart. It is very difficult to identify the subtle changes in the ECG in time and frequency domains. The Discrete Wavelet Transform (DWT) can provide good time and frequency resolutions and is able to decipher the hidden complexities in the ECG. In this study, five types of beat classes of arrhythmia as recommended by Association for Advancement of Medical Instrumentation (AAMI) were analyzed namely: non-ectopic beats, supra-ventricular ectopic beats, ventricular ectopic beats, fusion betas and unclassifiable and paced beats. Three dimensionality reduction algorithms; Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) and Independent Component Analysis (ICA) were independently applied on DWT sub bands for dimensionality reduction. These dimensionality reduced features were fed to the Support Vector Machine (SVM), neural network (NN) and probabilistic neural network (PNN) classifiers for automated diagnosis. ICA features in combination with PNN with spread value (Ïƒ) of 0.03 performed better than the PCA and LDA. It has yielded an average sensitivity, specificity, positive predictive value (PPV) and accuracy of 99.97\%, 99.83\%, 99.21\% and 99.28\% respectively using ten-fold cross validation scheme.},
	number = {5},
	urldate = {2017-05-03},
	journal = {Biomedical Signal Processing and Control},
	author = {Martis, Roshan Joy and Acharya, U. Rajendra and Min, Lim Choo},
	month = sep,
	year = {2013},
	keywords = {Association for Advancement of Medical Instrumentation (AAMI), Discrete Wavelet Transform (DWT), Electrocardiogram (ECG), Independent Component Analysis (ICA), Linear Discriminant Analysis (LDA), Principal Component Analysis (PCA), Support Vector Machine (SVM)},
	pages = {437--448},
	file = {ScienceDirect Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/R9ZGB5J5/S1746809413000062.html:text/html}
}

@inproceedings{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
	urldate = {2017-05-07},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
	file = {Full Text PDF:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/6TCTGJUX/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/T3ZI9HJE/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html:text/html}
}

@article{miyamoto_gated_2016,
	title = {Gated {Word}-{Character} {Recurrent} {Language} {Model}},
	url = {http://arxiv.org/abs/1606.01700},
	abstract = {We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.},
	urldate = {2017-05-07},
	journal = {arXiv:1606.01700 [cs]},
	author = {Miyamoto, Yasumasa and Cho, Kyunghyun},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01700},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1606.01700 PDF:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/IXI4MENC/Miyamoto and Cho - 2016 - Gated Word-Character Recurrent Language Model.pdf:application/pdf;arXiv.org Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/M26XP9P2/1606.html:text/html}
}

@misc{chris_olah_understanding_nodate,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2017-05-07},
	journal = {Understanding LSTM networks},
	author = {Chris Olah},
	file = {Understanding LSTM Networks -- colah's blog:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/EXMP7N9Q/2015-08-Understanding-LSTMs.html:text/html}
}

@inproceedings{long_fully_2015-1,
	title = {Fully convolutional networks for semantic segmentation},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
	urldate = {2017-05-07},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
	file = {long_shelhamer_fcn.pdf:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/ACGI48EQ/long_shelhamer_fcn.pdf:application/pdf}
}

@article{vinyals_matching_2016,
	title = {Matching {Networks} for {One} {Shot} {Learning}},
	url = {file:///home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/47TNEP77/1606.html},
	urldate = {2017-05-07},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	month = jun,
	year = {2016},
	file = {Full Text PDF:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/N3RP7X5Z/Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf:application/pdf;Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/K7A4U2IM/1606.html:text/html}
}

@article{santoro_one-shot_2016,
	title = {One-shot {Learning} with {Memory}-{Augmented} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1605.06065},
	abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
	urldate = {2017-05-07},
	journal = {arXiv:1605.06065 [cs]},
	author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06065},
	keywords = {Computer Science - Learning},
	annote = {Comment: 13 pages, 8 figures},
	file = {arXiv\:1605.06065 PDF:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/J26EGKB8/Santoro et al. - 2016 - One-shot Learning with Memory-Augmented Neural Net.pdf:application/pdf;arXiv.org Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/X64AS7U8/1605.html:text/html}
}

@article{quang_dann:_2015,
	title = {{DANN}: a deep learning approach for annotating the pathogenicity of genetic variants},
	volume = {31},
	issn = {1367-4811},
	shorttitle = {{DANN}},
	doi = {10.1093/bioinformatics/btu703},
	abstract = {Annotating genetic variants, especially non-coding variants, for the purpose of identifying pathogenic variants remains a challenge. Combined annotation-dependent depletion (CADD) is an algorithm designed to annotate both coding and non-coding variants, and has been shown to outperform other annotation algorithms. CADD trains a linear kernel support vector machine (SVM) to differentiate evolutionarily derived, likely benign, alleles from simulated, likely deleterious, variants. However, SVMs cannot capture non-linear relationships among the features, which can limit performance. To address this issue, we have developed DANN. DANN uses the same feature set and training data as CADD to train a deep neural network (DNN). DNNs can capture non-linear relationships among features and are better suited than SVMs for problems with a large number of samples and features. We exploit Compute Unified Device Architecture-compatible graphics processing units and deep learning techniques such as dropout and momentum training to accelerate the DNN training. DANN achieves about a 19\% relative reduction in the error rate and about a 14\% relative increase in the area under the curve (AUC) metric over CADD's SVM methodology.
AVAILABILITY AND IMPLEMENTATION: All data and source code are available at https://cbcl.ics.uci.edu/public\_data/DANN/.},
	language = {eng},
	number = {5},
	journal = {Bioinformatics (Oxford, England)},
	author = {Quang, Daniel and Chen, Yifei and Xie, Xiaohui},
	month = mar,
	year = {2015},
	pmid = {25338716},
	pmcid = {PMC4341060},
	keywords = {Algorithms, Area Under Curve, Computer Graphics, Genetic Variation, Genome, Human, Humans, Molecular Sequence Annotation, Neural Networks (Computer), Selection, Genetic, Support Vector Machine},
	pages = {761--763}
}

@article{ganin_domain-adversarial_2015,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	urldate = {2017-05-07},
	journal = {arXiv:1505.07818 [cs, stat]},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, FranÃ§ois and Marchand, Mario and Lempitsky, Victor},
	month = may,
	year = {2015},
	note = {arXiv: 1505.07818},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Published in JMLR: http://jmlr.org/papers/v17/15-239.html},
	file = {arXiv\:1505.07818 PDF:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/KWPDSRXJ/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/DN56Z96P/1505.html:text/html}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} method for stochastic optimization},
	shorttitle = {Adam},
	url = {https://arxiv.org/abs/1412.6980},
	urldate = {2017-05-08},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik and Ba, Jimmy},
	year = {2014},
	file = {1412.pdf:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/S7KMIBPE/1412.pdf:application/pdf}
}

@inproceedings{wan_regularization_2013,
	title = {Regularization of neural networks using dropconnect},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013_wan13},
	urldate = {2017-05-08},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning} ({ICML}-13)},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann L. and Fergus, Rob},
	year = {2013},
	pages = {1058--1066},
	file = {Regularization of Neural Networks using DropConnect - icml2013.pdf:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/6HUDQERI/icml2013.pdf:application/pdf}
}

@misc{noauthor_understanding_nodate,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2017-05-08},
	file = {Understanding LSTM Networks -- colah's blog:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/AA4P3Z2J/2015-08-Understanding-LSTMs.html:text/html}
}

@misc{noauthor_tensorflow:_nodate,
	title = {{TensorFlow}: {How} to optimise your input pipeline with queues and multi-threading},
	url = {https://blog.metaflow.fr/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0},
	urldate = {2017-05-08},
	file = {TensorFlow\: How to optimise your input pipeline with queues and multi-threading:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/6ZXR6STP/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0.html:text/html}
}

@misc{noauthor_performance_nodate,
	title = {Performance {Guide}},
	url = {https://www.tensorflow.org/performance/performance_guide},
	urldate = {2017-05-08},
	journal = {TensorFlow},
	file = {Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/ZG79H86D/performance_guide.html:text/html}
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}â„¢ {Patterns}},
	url = {http://www.tensorflowpatterns.org/},
	urldate = {2017-05-08},
	file = {TensorFlowâ„¢ Patterns:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/RPBU8QD8/www.tensorflowpatterns.org.html:text/html}
}

@misc{noauthor_cs231n_nodate,
	title = {{CS}231n {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/},
	urldate = {2017-05-08},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/KGR2G5QU/cs231n.github.io.html:text/html}
}

@misc{noauthor_wildml_nodate,
	title = {{WildML} â€“ {AI}, {Deep} {Learning}, {NLP}},
	url = {http://www.wildml.com/},
	urldate = {2017-05-08},
	file = {WildML â€“ AI, Deep Learning, NLP:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/INT92S5T/www.wildml.com.html:text/html}
}

@misc{noauthor_tutorials_nodate,
	title = {Tutorials},
	url = {https://www.tensorflow.org/tutorials/},
	urldate = {2017-05-08},
	journal = {TensorFlow},
	file = {Snapshot:/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/M3JUKZJ4/tutorials.html:text/html}
}

@misc{noauthor_learning_nodate,
	title = {Learning {TensorFlow} ::},
	url = {http://learningtensorflow.com/index.html},
	urldate = {2017-05-08},
	file = {Learning TensorFlow \:\::/home/csbotos/.mozilla/firefox/l9sv74v9.default/zotero/storage/IK2WHMAB/index.html:text/html}
}