\section{My commitments to the project}

I aim to generalize this writing and be less specific about the exact project we are working on, so the following experiences can be transferred to other fields of otherwise non related studies.
Basically our task was a simple classification problem for time samples of various length.
My task was not just to try out famous Machine Learning concepts which \it{may} work, but to support our research group with the necessary backend of the experiments carried out, and to integrate different results in the final unified model.

\subsection{Environment}

For doing so I vouched for using TensorFlow environment, in which I had previous experience \cite{github-projects}.
For our project we started by applying different previously proven to be succesful feature extraction methods, and for introducing Deep Learning we wanted to improve our progress by building on top of these features.
So in the first step I had to break down the recent DL architectures I wanted to use to basic modules.
When reimplementing them I paid attention to make spaceholders or entry points for every possible external features my colleagues developed.

Entry points at different level of processing the input can be categorized as the following:
- Variable length features (i.e. output of filters).
- Fixed representation features (i.e. variability indices, frequency domain components).
- Suggestions for classes (i.e. external classifier suggestion)

Also, to improve the model's efficiency I applied queues after different entry points to make mini-batch processing available, by filling these queues independently in mutliple threads, so the network trainer do not have to wait for these external sources to finish their processes to update the network's weights.

\subsection{Data standardization}

The biggest obstacles at first sight in using TensorFlow are the Tensors and the (control) Flow itself. In order to deploy a model we have to first assemble its computational graph, which will be compiled at run-time so it can be fed with real NumPy arrays and evaluated in the most efficient way relying on the TensorFlow backend.

Knowing that any time we feed values in, and retreive values from Tensors the whole process has to switch back and forth between C++/CUDA backend and the Python interface, which in case of large arrays is computationally expensive, and possibly collapse the parallel processes to sequential evaluation causing unnecessary run-time overhead, making TensorFlow seemingly inefficient.
In order to achieve best performance we have decrease the number of python API calls of Tensor evaluation, to let the backend order its computations in one run.

Special case of reducing calls is to discard the notorius use of placeholders \cite{placeholder-remove-MEDIUM-blog}, and introduce Queues to the model.

TensorFlow comes with a handful of convenience functions and wrappers which can handle data loading and preprocessing in parallel threads, controlled by a native thread coordinator.

These operators can be built in the graph without actually loading the data, and provide Tensor outputs which instead of placeholders will feed the network automatically on evaluation, so we can forget the \textit{feed dict}.
By using queues we can ensure that the main training process is running continously by chopping minibatches off the queues' front while new samples are fed back to the other end, possibly on different devices and multiple threads.
As a result the main process will not starve on input data, neither will be continously interrupted by Python calls to simply just transfer values from the API to the backend.

For the reasons above I prepared a \textit{writer.py} script that takes our raw data files, reads them into memory and write them out in a single file in the suggested file format, \textit{.TFRecord}. These files can be managed more easily and preserve the sufficient data in a compact way.
