\section{Introduction}

Focusing on recent succes of different Deep Learning architectures, one can get easily intimidated to implement them for individual purposes.
However many experiments are failing due to wrong choice of models, or not being able to track down the right hyperparameters.
There is no theorem (yet) for answering whether a network will converge or not, even if it does so, it may fall in a local minima of the search space, that is not satisfactory.
Following thumb rules \cite{thumbrules?}, can reduce our search space, still we have to be able to track our progress, and make every experiments reproduceable.
Also if large computational capacity is present, it is still not trivial how to exploit our sources. To make the best use of our given background we decided to use a framework that is transparent in such a manner, that we don't have to rewrite our codebase if only CPUs are available, multiple GPUs or even cloud computing clusters are at dispose. For these reasons we chose TensorFlow, which my work was focused on and this writing is based on.

Across the many implementations I have met in open source projects \cite{OSS?} and online tutorials \cite{wildml - metaflow - etc}, I tried to decide which of them would fit the best our needs.
Thankfully the great camp of enthusiasts and support of the library has developed a site for gathering the most popular patterns to use when it comes to actually writing the tests \cite{tensorflow-patterns}.

The main goal of this writing is to describe the process step-by-step of developing the technical background of a succesful Deep Learning product, where I present what I learned from my personal experiences.
